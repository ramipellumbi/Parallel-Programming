\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{booktabs}
\usepackage{forest}
\usepackage{tabu}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{color}
\usepackage{multicol}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{subfig}
\usepackage[font=small,labelfont=bf]{caption} % Required for specifying 
                                              % captions to tables and figures

\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{CPSC 524 Assignment 3: Parallel Matrix Multiplication}
\author{Rami Pellumbi\thanks{M.S., Statistics \& Data Science}}
\date{\today}

\begin{document}

\maketitle

\newpage 

\section{Introduction}

This assignment investigates the implementation and performance of an MPI-
based program tasked with parallel computation of large double precision matrices.
The program's objective is to effectively utilize multiple processors to perform 
the multiplication, optimizing the use of system resources and minimizing 
computation time. The implementation required careful consideration of data 
distribution and process synchronization, ensuring accuracy while maximizing 
computational efficiency. Performance analysis focuses on the scalability of the solution across various 
matrix sizes and the number of processors used. This report outlines the design 
considerations, describes the implementation strategy, and presents an 
evaluation of the program's computational performance.

\section{Project Organization}

The project is laid out as follows:

\begin{multicols}{2}
    \begin{forest}
        for tree={
            font=\ttfamily,
            grow'=0,
            child anchor=west,
            parent anchor=south,
            anchor=west,
            calign=first,
            edge path={
                \noexpand\path [draw, \forestoption{edge}]
                (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child anchor)\forestoption{edge label};
            },
            before typesetting nodes={
                if n=1
                {insert before={[,phantom]}}
                {}
            },
            fit=band,
            before computing xy={l=15pt},
        }
    [
        [docs/]
        [part1
            [Makefile]
            [runtask1.sh]
            [runtask2.sh]
            [runtask3.sh]
            [task1.c]
            [task2.c]
            [task3.c]
        ]
        [part2
            [bin/]
            [include/]
            [out/]
            [src/]
            [build-run-mpi.sh]
            [build-run-serial.sh]
            [build-run-task5.sh]
            [build-run-task6.sh]
            [build-run-task7.sh]
            [build-run-task8.sh]
            [Makefile]
        ]
    ]
    \end{forest}
    \columnbreak
    \begin{itemize}
        \item \textbf{docs/}: This folder contains LaTeX files and other documentation materials that pertain to the report.
        \item \textbf{part1/}: Code for MPI basics and part 1 tasks.
        \item \textbf{part2/}: Code for parallel matrix multiplication.
        \begin{itemize}
            \item \textbf{bin/}: The \texttt{bin} folder holds compiled objects and executable files, centralizing the output of the compilation process.
            \item \textbf{include/}: Here, all the header files (\texttt{.h}) are stored. 
            \item \textbf{out/}: The \texttt{out} folder stores the outputs from each task. It also houses the csv file containing data generated by the programs.
            \item \textbf{src/}: This directory houses the source files (\texttt{.c}) that make up the benchmarks.
            \item \textbf{Shell Scripts}: The shell scripts are used to submit the job for the relevant task to slurm via \texttt{sbatch}.  
        \end{itemize}
    \end{itemize}
\end{multicols}

\section{Code Explanation, Compilation, and Execution}

This section outlines the steps required to build and execute the code. The provided Bash scripts automate the entire process, 
making it straightforward to compile and run the code. All the below steps assume 
you are in the root of the project directory.

\subsection{Automated Building and Execution}
\begin{itemize}
    \item \textbf{Part 1: Understanding MPI Basics} 
    All part 1 related code is in the \texttt{part1/} directory.
    There are three scripts for part 1, one for each task
    The commands \texttt{sbatch runtask1.sh}, \texttt{sbatch runtask2.sh}, 
    and \texttt{sbatch runtask3.sh} will compile and run the files 
    \texttt{task1.c}, \texttt{task2.c}, and \texttt{task3.c}, respectively.
    The output of the experiments will be in \texttt{out/serial.csv}.
    
    \item \textbf{Part 2: Parallel Matrix Multiplication}
    All part 2 related code is in the \texttt{part2/} directory.
    There are multiple programs run in Part 2: 
    \begin{itemize}
        \item \texttt{serial.c} Simple, serial, matrix multiplication.
        \item \texttt{task5.c}: Parallel matrix multiplication of square matrices with blocking collectives. 
        Assumes that the number of rows is evenly divisible by the number of processes.
        \item \texttt{task6.c}: Parallel matrix multiplication of square matrices with non blocking collectives. 
        Assumes that the number of rows is evenly divisible by the number of processes.
        \item \texttt{task7.c}:  Parallel matrix multiplication of square matrices with non blocking collectives, 
        with a focus on interleaving communication and computation. Assumes that the number of rows is evenly divisible by the number of processes.
        \item \texttt{taks8.c} Parallel matrix multiplication of square matrices with blocking collectives. 
        Appropriately handles when the number of rows is not evenly divisible by the number of processes.
    \end{itemize}
    To run all the MPI experiments, execute \texttt{sbatch build-run-mpi.sh}. The results are stored in 
    \texttt{out/omp.csv}.
\end{itemize}

\subsection{Post-Build Objects and Executables}
For part 2, upon successful compilation and linking, an \texttt{obj/} subdirectory will be generated within the directory. 
This directory will contain the compiled output files. Additionally, the executable files for running each part will be 
situated in the \texttt{bin/} subdirectory.

\subsection{Output Files From \texttt{sbatch}}
For part 1, the output files generated from running the code by submitting the relevant Bash script via \texttt{sbatch} will be 
in the \texttt{part1} directory. 
For part 2, the output files generated from running the code by submitting the relevant Bash script via \texttt{sbatch} will be 
stored in the \texttt{out} directory. 

\section{MPI Basics}

\section{Parallel Matrix Multiplication}
At a high level, we have input $N \times N$ matrices $A$ and $B$, and an output $N \times N$ matrix $C$..
The input matrices, \(A\) and \(B\), are divided into blocks of rows and columns, respectively. 
Each block row of matrix \(A\) is then multiplied with the corresponding block column of matrix \(B\) 
to compute a segment of the resultant matrix \(C\). This block-wise computation enables the distribution 
of the workload among the available processors in the MPI environment. The distribution and collection of data blocks 
utilize MPI's collective communication operations, ensuring that each processor has the necessary data to perform 
its assigned calculations. The final step involves aggregating the computed blocks from all processors to form the complete resultant matrix.
The implementation of the parallel matrix multiplication algorithm follows the ring-pass strategy, which is outlined as follows for the case where $N$ is divisible by $p$ (the number of processes):
\begin{enumerate}
    \item The manager initializes memory of $A$, $B$, and $C$ and initializes $A$ and $B$ with the data to be multiplied.
    \item The manager partitions $A$ and $C$ into $p$ blocks rows each. It partitions $B$ into $p$ block columns.
    \begin{itemize}
        \item $A$ and $C$ are of dimension $(N/p) \times N$.
        \item $B$ is of dimension $N \times (N/p)$.
    \end{itemize}
    \item The manager permanently assigns to each MPI process one block row each of $A$ and $C$, and it assigns 
    each MPI process one block column of $B$ as its initial assignment.
    \item The computation proceeds iteratively until completed:
    \begin{enumerate}
        \item Each MPI process does all the computation it can with the data at hand.
        \item MPI processes then pass their block columns of $B$ to the next higher-ranked MPI process.
    \end{enumerate}
    \item The manager uses MPI collective operations to assemble the full $C$ matrix by collecting 
    all block rows of $C$ from the other MPI processes.
\end{enumerate}

\subsection*{Task 5: Blocking Collectives}
The first attempt at this algorithm was utilizing MPI's blocking collectives. Namely, 
\begin{enumerate}
    \item The manager process initializes matrices $A$ and $B$ with random double
    precision values and creates an empty matrix $C$ to store the result.
    \item \texttt{MPI\_Scatter} is used to distribute the blockes of $A$, $B$, and $C$ across all processes.
    \item Each process computes a partial result by multiplying its block of A with the corresponding block of B and stores it in \texttt{multiply\_result}.
    \item The partial results are stored in the local block of matrix C, \texttt{blockC}.
    \item Processes then pass their block of $B$ to the next process using \texttt{MPI\_Send} and \texttt{MPI\_Recv}.
    \begin{itemize}
        \item To avoid deadlock, even-ranked processes send before they receive and odd-ranked processes receive before they send.
        \item Each process makes use of a secondary \texttt{blockB} buffer, named \texttt{tempB}, else a process may overwrite its initial segment of $B$ before it is able to use it in a matrix multiplication. 
        This allows one buffer to send while the other is being used in an operation.
    \end{itemize}
    \item After all computations complete, the processes gather their computed blocks into $C$ using \texttt{MPI\_Gather}.
\end{enumerate}
The results of this approach are in Table 1. 
\begin{table}[H]
    \centering
    \caption{Blocking Collectives - Average Performance (s)}
    \fontsize{12}{14}\selectfont
    \begin{tabular}[t]{rrrrr}
    \toprule
    p/N  & 1000 & 2000 & 4000 & 8000\\
    \midrule
    1 & 0.313 & 2.806 & 39.271 & 327.932\\
    2 & 0.210 & 1.697 & 21.191 & 169.081\\
    4 & 0.141 & 0.836 & 10.058 & 85.685\\
    8 & 0.093 & 0.517 & 4.083 & 43.770\\
    \bottomrule
    \end{tabular}
    \caption*{Comparing MPI blocking collectives to serial performance as process count increases.}
\end{table}
\noindent Graphically:
\begin{figure}[H]
   \centering
   \includegraphics[scale=0.7]{../part2/out/np.pdf} 
   \caption{Blocking Communication: Log2 Performance vs. Number of Processes}
\end{figure}
\noindent The dashed line represents a doubling in performance (halving the runtime) 
as the number of processes doubles relative to the serial runtime. We see that 
for increasing $N$, we achieve closer to the desired throughput via scaling. 
For $N = 1000$, increasing the number of prcesses increases performance but at a 
smaller magnitude relative to larger $N$. It is likely that the overhead in 
communication here is to blame. As $N$ increases, doubling the process count 
just about doubles performance. It seems as though the computational load is 
heavy enough that the communication overhead becomes a smaller fraction of 
the total runtime, allowing for better scalability. That is, the serial portion of a task 
(in this case, the communication overhead) becomes negligible as the problem size increases.

\subsection*{Task 6: Non Blocking Collectives}
Next, we simply replace our blocking collectives with non blocking collectives. 
The algorithm and ring passing stayed entirely the same. What changes was:
\begin{itemize}
    \item \texttt{MPI\_Irecv} and \texttt{MPI\_Isend} instead of \texttt{MPI\_Recv} and \texttt{MPI\_Send}, respectively.
    \item \texttt{MPI\_Iscatter} and \texttt{MPI\_Igather} instead of \texttt{MPI\_Scatter} and \texttt{MPI\_Gather}, respectively.
    \item Adding \texttt{MPI\_Waitall} and \texttt{MPI\_Wait} as necessary, e.g., to ensure the blocks have all been scattered.
\end{itemize}
The performance is seen in Table 2 and Figure 2.
\begin{table}[H]
    \centering
    \caption{Non Blocking Collectives - Average Performance (s)}
    \fontsize{12}{14}\selectfont
    \begin{tabular}[t]{rrrrr}
    \toprule
    p/N & 1000 & 2000 & 4000 & 8000\\
    \midrule
    1 & 0.313 & 2.806 & 39.271 & 327.932\\
    2 & 0.206 & 1.653 & 21.036 & 168.987\\
    4 & 0.128 & 0.809 & 9.908 & 85.196\\
    8 & 0.079 & 0.480 & 4.050 & 43.294\\
    \bottomrule
    \end{tabular}
    \caption*{Comparing MPI non blocking collectives to serial performance as process count increases.}
\end{table}
\begin{figure}[H]
   \centering
   \includegraphics[scale=0.7]{../part2/out/np-6.pdf} 
   \caption{Non Blocking Communication: Log2 Performance vs. Number of Processes}
\end{figure}
\noindent The performance assessment here is precisely that of the blocking collectives. Since 
very little was changed in terms of computation and communication, we expect performance 
to be about similar, as observed.

\subsection*{Task 7: Overlapping Computation and Communication}

\subsection*{Task 8: Dealing with Non Uniform Buffer Sizes}
Lastly, we modify the blocking collectives code to handle the case where $N$ 
is not divisible by $p$. The following had to be considered (in continuation from task 5):
\begin{itemize}
    \item $N/p$ will have a remainder, i.e., $N \% p$. This remainder 
    will be less than the total number of processes and so all ranks less than 
    the remainder will get $\mathsf{floor}(N/p) + 1$ rows (or columns) and the 
    rest of the ranks get $\mathsf{floor}(N/p)$ rows (or columns).
    \begin{itemize}
        \item Now, every \texttt{blockA}, \texttt{blockB}, or \texttt{blockC} has 
        array size $(\texttt{floor}(N/p) + 1) \times N$ or $\texttt{floor}(N/p) \times N$. 
    \end{itemize}
    \item For the initial assignments, $\texttt{MPI\_Scatterv}$ is used with the appropriately passed 
    in displacements array to distribute the blocks.
    \begin{itemize}
        \item Each process still gets a permanant allocation of \texttt{blockA} and \texttt{blockC} and 
        starts with an initial assignment of \texttt{blockB}.
        \item Each process allocates a size for \texttt{blockB} and \texttt{tempB} that is the largest possible size.
        In doing so, the differently sized buffers can be passed around without having to reallocate memory.
        \item At each time step on a process, a multiply might be between:
        \begin{enumerate}
            \item An $(\texttt{floor}(N/p) + 1) \times N$ \texttt{blockA} and an $N \times (\texttt{floor}(N/p) + 1)$ \texttt{blockB}.
            \item An $(\texttt{floor}(N/p) + 1) \times N$ \texttt{blockA} and an $N \times \texttt{floor}(N/p)$ \texttt{blockB}.
            \item An $\texttt{floor}(N/p) \times N$ \texttt{blockA} and an $N \times (\texttt{floor}(N/p) + 1)$ \texttt{blockB}.
            \item An $\texttt{floor}(N/p) \times N$ \texttt{blockA} and an $N \times \texttt{floor}(N/p)$ \texttt{blockB}.
        \end{enumerate}
        To handle this multiply, each process has an array \texttt{multiply\_result} of the maximum possible multiply result size that it allocates,
        i.e., $(\texttt{floor}(N/p) + 1) \times (\texttt{floor}(N/p) + 1)$.
    \end{itemize}
    \item At step $i$ in the ring swap, process $p$ is working on the \texttt{blockB} initially allocated to rank \texttt{(rank - i + p) \% p}.
    The size of each ranks initial allocation is kept track of to appropriately index the \texttt{blockB} array for multiplication 
    (since it is allocated to the maximum possible size it may have data in it we do not want). The computed \texttt{multiply\_result} is appropriately placed in \texttt{blockC}, offset 
    by the appropriate index relative to the initial allocations. 
\end{itemize}
The resulting code is an algorithm that produces 
    the following performance and $F$-norm on an $N = 7633$, $p = 7$ program:
    \begin{table}[H]
        \centering
        \caption{Non Uniform Size Allocation with Blocking Collectives}
        \fontsize{12}{14}\selectfont
        \begin{tabular}[t]{rrrr}
        \toprule
        p & N & time & F-norm\\
        \midrule
        7 & 7633 & 43.22639 & 1.535e-09\\
        \bottomrule
        \end{tabular}
\end{table}

\section{Conclusion}

\end{document}

