\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{booktabs}
\usepackage{forest}
\usepackage{tabu}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{color}
\usepackage{multicol}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{subfig}
\usepackage[font=small,labelfont=bf]{caption} % Required for specifying 
                                              % captions to tables and figures

\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{CPSC 524 Assignment 4: CUDA Matrix Multiplication}
\author{Rami Pellumbi\thanks{M.S., Statistics \& Data Science}}
\date{\today}

\begin{document}

\maketitle

\newpage 

\section{Introduction}
This assignment explores GPU programming using CUDA, centered around the task of matrix multiplication. 
The primary challenge involves constructing a CUDA kernel capable of 
multiplying two random rectangular matrices. This initial task serves as a 
gateway into the realms of parallel computing and efficient memory management, 
foundational elements in leveraging GPU architecture for computational tasks. 
As we progress, the report addresses more advanced techniques, such as the 
utilization of shared memory and the strategic optimization of thread 
computations, essential for enhancing the computational performance. 

\section{Project Organization}

The project is laid out as follows:

\begin{multicols}{2}
    \begin{forest}
        for tree={
            font=\ttfamily,
            grow'=0,
            child anchor=west,
            parent anchor=south,
            anchor=west,
            calign=first,
            edge path={
                \noexpand\path [draw, \forestoption{edge}]
                (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child anchor)\forestoption{edge label};
            },
            before typesetting nodes={
                if n=1
                {insert before={[,phantom]}}
                {}
            },
            fit=band,
            before computing xy={l=15pt},
        }
    [
        [docs/]
        [bin/]
        [out/]
        [src/
        [serial.cu]
        [task1.cu]
        ]
        [build-run-task1a.sh]
        [build-run-task1b.sh]
        [build-run-task1c.sh]
        [build-run-task2a.sh]
        [build-run-task2b.sh]
        [build-run-task3.sh]
        [build-run-task4.sh]
        [Makefile]
    ]
    \end{forest}
    \columnbreak
    \begin{itemize}
        \item \textbf{docs/}: This folder contains LaTeX files and other documentation materials that pertain to the report.
        \item \textbf{bin/}: The \texttt{bin} folder holds compiled objects and executable files, centralizing the output of the compilation process.
        \item \textbf{out/}: The \texttt{out} folder stores the outputs from each task. It also houses the csv file containing data generated by the programs.
        \item \textbf{src/}: This directory houses the source files (\texttt{.cu}) that make up the benchmarks.
        \item \textbf{Shell Scripts}: The shell scripts are used to submit the job for the relevant task to slurm via \texttt{sbatch}.  
    \end{itemize}
\end{multicols}

\section{Code Explanation, Compilation, and Execution}

This section outlines the steps required to build and execute the code. The provided Bash scripts automate the entire process, 
making it straightforward to compile and run the code. All the below steps assume 
you are in the root of the project directory.

\subsection{Automated Building and Execution}
All related code is in the \texttt{src/} directory.
There are multiple programs: 
\begin{multicols}{2}
    \begin{itemize}
        \item \texttt{task1.cu}
        \item \texttt{task2.cu}
        \item \texttt{task3.cu}
        \item \texttt{task4.cu}
    \end{itemize}
\end{multicols}
\noindent To run any one experiment, execute the relevant bash script, e.g., \texttt{build-run-task1a.sh}. It should be 
noted that for the \texttt{build-run-task1b.sh} and \texttt{build-run-task2b.sh} the \texttt{FP} must be defined to double on line 1 in 
the respective task file.

\subsection{Post-Build Objects and Executables}
Upon successful compilation and linking, an \texttt{obj/} subdirectory will be generated within the directory. 
This directory will contain the compiled output files. Additionally, the executable files for running each program will be 
situated in the \texttt{bin/} subdirectory.

\subsection{Output Files From \texttt{sbatch}}
The output files generated from running the code by submitting the relevant Bash script via \texttt{sbatch} will be 
stored in the \texttt{out} directory. 

\section{Task 1: CUDA Matrix Multiplication}
First, a CUDA kernel is built to handle the multiplication of two 
random rectangular matrices: $C = AB$, where $A$ is $n \times p$ and $B$ is $p \times m$. 
Global memory is used to store the matrices and 
perform the computation. Each GPU thread computes only a single element of $C$. 
The input matrices $A$ and $B$ are stored row-wise into a one-dimensional array, 
and the output matrix $C$ is stored in the same manner. In addition to the kernel, a host multiplication takes form using the $kij$ access pattern:
\begin{lstlisting}
for (int k = 0; k < p; k++) {
    size_t ia = i * p + k; // row i column k of A
    r = A[ia];
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < m; j++) {
            size_t ib = k * m + j; // row k column j of B
            size_t ic = i * m + j; // row i column j of C

            C[ic] += r * B[ib];
        }
    }
}
\end{lstlisting}

\subsection{GPU Kernel Performance: float}
The kernel was written to accept $7$ arguments: 
\begin{multicols}{2}
    \begin{itemize}
        \item \texttt{int n}: Number of rows in $A$ and $C$.
        \item \texttt{int p}: Number of columns in $A$ and rows in $B$.
        \item \texttt{int m}: Number of columns in $B$ and $C$.
        \item \texttt{int block\_dim\_x}: The tile size to use for the computation.
        \item \texttt{int block\_dim\_y}: The tile size to use for the computation.
        \item \texttt{grid\_dim\_x}: The number of blocks to use in the $x$-direction.
        \item \texttt{grid\_dim\_y}: The number of blocks to use in the $y$-direction.
    \end{itemize}
\end{multicols}
\noindent All device multiplication was validated in error performance against the serial 
code when able. This code was omitted from the individual task files for clarity, 
and the serial code put in its own \texttt{serial.cu} for clarity.
The following bash script outline was used in Task 1, allowing 
for control over varying the block and grid size (handling the appropriate 
setting of grid dimensions):
\newpage
\begin{lstlisting}
# n,m,p blocks delimited by a space
sizes="1024,1024,1024 8192,8192,8192 1024,1024,8192 8192,8192,1024 8192,1024,8192"

for tuple in $sizes
do
    IFS=',' read -ra ADDR <<< "$tuple"
    n=${ADDR[0]}
    m=${ADDR[1]}
    p=${ADDR[2]}

    echo "Running n=$n, p=$p, m=$m"
    for blockx in 8 16 32 64 128 256
    do
        blocky=$((1024/$blockx))
        Grid_Dim_x=$((($m + $blockx - 1)/$blockx))
        Grid_Dim_y=$((($n + $blocky - 1)/$blocky))
        echo "Running block BLOCK_DIM_X=$blockx BLOCK_DIM_Y=$blocky"
        echo "With GRID_DIM_X=$Grid_Dim_x GRID_DIM_Y=$Grid_Dim_y"
        time ./bin/task1 $n $p $m $blockx $blocky $Grid_Dim_x $Grid_Dim_y
        time ./bin/task1 $n $p $m $blockx $blocky $Grid_Dim_x $Grid_Dim_y
        time ./bin/task1 $n $p $m $blockx $blocky $Grid_Dim_x $Grid_Dim_y
    done
done
\end{lstlisting}
\noindent Modifications of this script are carried over for parts 2 through 4. It 
should be noted that in task 4 the \texttt{Grid\_Dim\_x} is adjusted to account for the varying \texttt{NTB} values (in 
the program instead of the bash script).
The performance table for the best performing (\texttt{block\_x, block\_y}) pair 
across the verying matrix sizes is:
\begin{table}[H]
    \centering
    \caption{GPU Kernel Performance Float (ms)}
    \fontsize{12}{14}\selectfont
    \begin{tabular}[t]{lllrr}
    \toprule
    (n, m, p) & (block x, blocky) & (grid x, grid y) & gpu time (ms) & cpu time (ms)\\
    \midrule
    (1024,1024,1024) & (64, 16) & (16, 64) & 3.628693 & 253.5439\\
    (1024,1024,8192) & (128, 8) & (8, 128) & 42.841162 & 2218.2016\\
    (8192,8192,1024) & (16, 64) & (512, 128) & 271.195058 & 28123.4727\\
    (8192,1024,8192) & (64, 16) & (16, 512) & 290.632660 & 25230.9069\\
    (8192,8192,8192) & (16, 64) & (512, 128) & 2179.101807 & 224239.7760\\
    \bottomrule
    \end{tabular}
\end{table}
\noindent We observe that the GPU offers significant speedup, up to about 100$\times$ in some cases

\subsection{GPU Kernel Performance: double}
We assess the performance of the GPU on doubles for square matrix $n=m=p=8192$. 
The results were:
\begin{table}[H]
    \centering
    \caption{GPU Kernel Performance Double (ms)}
    \fontsize{12}{14}\selectfont
    \begin{tabular}[t]{lllr}
    \toprule
    (n,m,p) & (block x, block y) & (grid x, grid y) & gpu time\\
    \midrule
    (8192,8192,8192) & (8,128) & (1024,64) & 3462.878\\
    (8192,8192,8192) & (16,64) & (512,128) & 2870.899\\
    (8192,8192,8192) & (32,32) & (256,256) & 2826.502\\
    (8192,8192,8192) & (64,16) & (128,512) & 2944.203\\
    (8192,8192,8192) & (128,8) & (64,1024) & 3005.359\\
    (8192,8192,8192) & (256,4) & (32,2048) & 3231.829\\
    \bottomrule
    \end{tabular}
\end{table}
\noindent We see that the best performing kernel was for block dimensions $(32,32)$ and grid dimensions $(256,256)$.
This is a change from the best performing block dimensions of the float kernel, which was $(16,64)$. The 
gpu execution time increased from $2179.101807$ to $2826.502$, a $29.7\%$ increase.

\subsection{Maximum Matrix Size}
The RTX 2080 Ti has 11,012 MegaBytes of memory. That is 
\begin{align*}
    11,012 MB \times \frac{1024\ KB}{MB} \times \frac{1024\ B}{KB} = 11546918912\ \mathsf{Bytes} := C.
\end{align*}
Each float carries $4$ bytes and thus the maximum matrix size that can be allocated is subject to the constraint
$$4\big(n \cdot p + p \cdot m + n \cdot m\big) \leq C.$$
For square matrices $n = p = m$, this reduces to 
$$4 \cdot 3 \cdot n^2 \leq C \implies n \leq \sqrt{\frac{C}{12}} = 31020$$
Of course, this is the maximum size that can be allocated, but the actual maximum size is subject to the memory
required by other processes. The maximum size that was able to be allocated was $n = 30536$. 
The timing result table is:
\begin{table}[H]
    \centering
    \caption{Max Size GPU Kernel Performance Float (ms)}
    \fontsize{12}{14}\selectfont
    \begin{tabular}[t]{lr}
    \toprule
    (n,m,p) & gpu time\\
    \midrule
    (30536,30536,30536) & 121486.5\\
    \bottomrule
    \end{tabular}
\end{table}

\section{Task 2: CUDA Matrix Multiplication with Shared Memory}
Next, we modify the kernel to use shared memory. For simplicity, 
we use \texttt{block\_dim\_x = block\_dim\_y = tile\_width}, but we do 
not assume that the matrix dimensions are multiples of the tile width. 
The algorithm details are implemented entirely based on Kirk and Hwu's 
``Programming Massively Parallel Processors''. The best performing gpu time 
results are as follows (juxtaposed with the cpu time):
\begin{table}[H]
    \centering
    \caption{Shared Memory GPU Kernel Performance Float (ms)}
    \fontsize{12}{14}\selectfont
    \begin{tabular}[t]{lllrr}
    \toprule
    (n,m,p) & (block x, block y) & (grid x, grid y) & gpu time (ms) & cpu time (ms)\\
    \midrule
    (1024,1024,1024) & (16,16) & (64,64) & 2.154752 & 253.5439\\
    (1024,1024,8192) & (16,16) & (64,64) & 12.527520 & 2218.2016\\
    (8192,8192,1024) & (16,16) & (512,512) & 105.650368 & 28123.4727\\
    (8192,1024,8192) & (16,16) & (64,512) & 131.119908 & 25230.9069\\
    (8192,8192,8192) & (32,32) & (256,256) & 885.436859 & 224239.7760\\
    \bottomrule
    \end{tabular}
\end{table}
\noindent We remark that there is a significant performance improvement from exploiting 
shared memory via tiled matrix multiplication. The best case tiled version 
significantly outperforms the best case non-tiled version.

\section{Task 3: Reducing Tile Loads}
Per the assignment, an important algorithmic decision in performance tuning 
is the granularity of thread computations. We attempt to improve performance 
by implementing a multi-tiled matrix multiplication kernel. The implementation was 
as follows:
\begin{lstlisting}
 __global__ void gpu_mult(FP *A, FP *B, FP *C, int n, int p, int m, int TILE_WIDTH)
{
    extern __shared__ FP tiles[];
    FP *Ads = &tiles[0]; // TILE_WIDTH x TILE_WIDTH
    FP *Bds[NTB];        // pointer to an array of NTB elements each of size TILE_WIDTH x TILE_WIDTH

    
    FP cvalues[NTB]; // initialize cvalues
    for (size_t kt = 0; kt < NTB; kt++)
    {
        // offset to appropriate shared memory location
        Bds[kt] = &tiles[(kt + 1) * TILE_WIDTH * TILE_WIDTH]; 
        cvalues[kt] = 0.;
    }

    int bx = blockIdx.x; int by = blockIdx.y;
    int tx = threadIdx.x; int ty = threadIdx.y;

    int row = by * TILE_WIDTH + ty;     // col needs to be dynamically computed based on NTB idx
    int tile_idx = ty * TILE_WIDTH + tx;

    for (size_t ph = 0; ph < ceil((double)p / (double)TILE_WIDTH); ph++)
    {
        int col_bound_A = ph * TILE_WIDTH + tx;
        int row_bound_B = ph * TILE_WIDTH + ty;

        if (row < n && col_bound_A < p) // load Ads
        {
            int indexa = row * p + col_bound_A;
            Ads[tile_idx] = A[indexa];
        } else { Ads[tile_idx] = 0.; }

        for (size_t kt = 0; kt < NTB; kt++) // load multiple tiles of B into the Bds array
        {
            int col_offset = tx + NTB * blockDim.x * bx + kt * TILE_WIDTH;
            int indexb = row_bound_B * m + col_offset;

            if (col_offset < m && row_bound_B < p)
            {
                Bds[kt][tile_idx] = B[indexb];
            } else { Bds[tile_idx] = 0.; }
        }
        __syncthreads();

        for (size_t k = 0; k < TILE_WIDTH; k++)
            for (size_t kt = 0; kt < NTB; kt++)
            {
                cvalues[kt] += Ads[ty * TILE_WIDTH + k] * Bds[kt][k * TILE_WIDTH + tx];
            }
        __syncthreads();
    }

    for (size_t kt = 0; kt < NTB; kt++)
    {
        int col_offset = tx + NTB * blockDim.x * bx + kt * TILE_WIDTH;
        if (col_offset < m && row < n) C[row * m + col_offset] = cvalues[kt];
    }
}
\end{lstlisting}
For simplicity of memory management, \texttt{NTB} is defined as a constant at 
compile time, else we would have had to index into the appropriate shared memory 
index of \texttt{tiles} - which was getting to be annoying.
The results for $n = m = p = 8192$ and a tile size of 32 across varying \texttt{NTB} values were:
\begin{table}[H]
    \centering
    \caption{Multi Tiled GPU Kernel Performance Float (ms)}
    \fontsize{12}{14}\selectfont
    \begin{tabular}[t]{rlllr}
    \toprule
    NTB & (n,m,p) & (block x block y) & (grid x, grid y) & gpu time (ms)\\
    \midrule
    8 & (8192,8192,8192) & (32,32) & (32,256) & 409.9855\\
    7 & (8192,8192,8192) & (32,32) & (37,256) & 397.8701\\
    6 & (8192,8192,8192) & (32,32) & (43,256) & 413.3653\\
    4 & (8192,8192,8192) & (32,32) & (64,256) & 467.5647\\
    2 & (8192,8192,8192) & (32,32) & (128,256) & 611.9062\\
    1 & (8192,8192,8192) & (32,32) & (256,256) & 896.6119\\
    \bottomrule
    \end{tabular}
\end{table}
\noindent The best performing kernel was for \texttt{NTB = 7}, with a gpu time of $397.8701$ ms.

\section{Task 4: Handling All Tile Sizes}
The task 3 program was implemented for the general case. Running it for $n=p=m=1100$ and 
a tile size of $16\times 16$ and 3 adjacent tiles yielded:
\begin{verbatim}
***Building task 4
rm -f obj/*.o  bin/task1  bin/task4  bin/task2  bin/task3  bin/serial 
nvcc -gencode=arch=compute_75,code=sm_75 -O3 -g -I include -o obj/serial.o -c src/serial.cu
Building binary bin/serial from object obj/serial.o
nvcc -gencode=arch=compute_75,code=sm_75 -lm -g -o bin/serial obj/serial.o
rm obj/serial.o
nvcc -gencode=arch=compute_75,code=sm_75 -O3 -g -I include -o obj/task4.o -c src/task4.cu
Building binary bin/task4 from object obj/task4.o
nvcc -gencode=arch=compute_75,code=sm_75 -lm -g -o bin/task4 obj/task4.o
rm obj/task4.o
Running n=1100, p=1100, m=1100
Running block BLOCK_DIM_X=16 BLOCK_DIM_Y=16
With GRID_DIM_X=69 GRID_DIM_Y=69
Device count = 1
Using device 0
Matrix Dimension = 1100
Block_Dim_x = 16, Block_Dim_y = 16, Grid_Dim_x = 23, Grid_Dim_y = 69
Time to calculate results on GPU: 1.652032 ms.
Time to calculate results on CPU: 311.685333 ms.
Approximate relative error between GPU and CPU: 4.315665e-08
\end{verbatim}
We note that the relative error is very small, and the gpu time is significantly
less than the cpu time.

\section{Conclusion}
In this report, we investigated the utilization of GPU programming using CUDA for matrix multiplication, 
revealing significant performance advantages over traditional CPU-based approaches. 
Our exploration highlighted the efficiency gains achievable through parallel processing inherent in GPU architecture. 
We further demonstrated how leveraging shared memory and tiling techniques can optimize GPU computations. 
Delving into advanced optimization with multi-tiled matrix multiplication, we observed 
that reducing the global memory accesses can significantly improve performance.
\end{document}

